{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\n",
      " tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]]) torch.Size([3, 1])\n",
      " tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]]) torch.Size([3, 1])\n",
      "2. tensor([0.], requires_grad=True) tensor([0.], requires_grad=True) 0.0 0.0\n",
      "3.\n",
      " tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n",
      "4. 4.666666507720947\n",
      "5. 0.09333334118127823 0.03999999910593033\n",
      "6.\n",
      " tensor([[0.1333],\n",
      "        [0.2267],\n",
      "        [0.3200]], grad_fn=<AddBackward0>)\n",
      "7. 3.6927406787872314\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "count=1\n",
    "x_train=torch.FloatTensor([[1], [2], [3]])\n",
    "y_train=torch.FloatTensor([[1], [2], [3]])\n",
    "W=torch.zeros(1,requires_grad=True)\n",
    "b=torch.zeros(1,requires_grad=True)\n",
    "hypothesis=x_train*W+b\n",
    "cost=torch.mean((hypothesis-y_train)**2)\n",
    "\n",
    "print(\"{}.\\n {} {}\\n {} {}\".format(count,x_train,x_train.shape,y_train,y_train.shape)) # 1. 학습데이터\n",
    "count+=1\n",
    "print(\"{}. {} {} {} {}\".format(count,W,b,W.item(),b.item())) # 2. 선형모델 기울기와 절편 (tensor.item() 값만 가져오기[*단일 tensor만 가능])\n",
    "count+=1\n",
    "print(\"{}.\\n {}\".format(count,hypothesis)) # 3. y_train의 예측값\n",
    "count+=1\n",
    "print(\"{}. {}\".format(count,cost)) # 4. y_train과 예측값의 오차\n",
    "count+=1\n",
    "\n",
    "optimizer=torch.optim.SGD([W, b], lr=0.01)\n",
    "optimizer.zero_grad()\n",
    "cost.backward()\n",
    "optimizer.step()\n",
    "\n",
    "hypothesis=x_train*W+b\n",
    "cost=torch.mean((hypothesis-y_train)**2)\n",
    "print(\"{}. {} {}\".format(count,W.item(),b.item())) # 5. 학습 후 선형모델 기울기와 절편\n",
    "count+=1\n",
    "print(\"{}.\\n {}\".format(count,hypothesis)) # 6. 학습 후 y_train의 예측값\n",
    "count+=1\n",
    "print(\"{}. {}\".format(count,cost)) # 7. 학습 후 y_train과 예측값의 오차\n",
    "count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.093, b: 0.040 Cost: 4.666667\n",
      "Epoch  100/1000 W: 0.873, b: 0.289 Cost: 0.012043\n",
      "Epoch  200/1000 W: 0.900, b: 0.227 Cost: 0.007442\n",
      "Epoch  300/1000 W: 0.921, b: 0.179 Cost: 0.004598\n",
      "Epoch  400/1000 W: 0.938, b: 0.140 Cost: 0.002842\n",
      "Epoch  500/1000 W: 0.951, b: 0.110 Cost: 0.001756\n",
      "Epoch  600/1000 W: 0.962, b: 0.087 Cost: 0.001085\n",
      "Epoch  700/1000 W: 0.970, b: 0.068 Cost: 0.000670\n",
      "Epoch  800/1000 W: 0.976, b: 0.054 Cost: 0.000414\n",
      "Epoch  900/1000 W: 0.981, b: 0.042 Cost: 0.000256\n",
      "Epoch 1000/1000 W: 0.985, b: 0.033 Cost: 0.000158\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x_train=torch.FloatTensor([[1], [2], [3]])\n",
    "y_train=torch.FloatTensor([[1], [2], [3]])\n",
    "W=torch.zeros(1, requires_grad=True)\n",
    "b=torch.zeros(1, requires_grad=True)\n",
    "optimizer=torch.optim.SGD([W, b], lr=0.01)\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    hypothesis=x_train*W+b\n",
    "    cost=torch.mean((hypothesis-y_train)**2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\n",
      " tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]]) torch.Size([3, 1])\n",
      " tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]]) torch.Size([3, 1])\n",
      "2.\n",
      " tensor([[-0.5580],\n",
      "        [-0.2719],\n",
      "        [ 0.0143]], grad_fn=<AddmmBackward>)\n",
      "3. 5.501039981842041\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "count=1\n",
    "class LinearRegressionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear=torch.nn.Linear(1,1) # (input,output)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "\n",
    "x_train=torch.FloatTensor([[1],[2],[3]])\n",
    "y_train=torch.FloatTensor([[1],[2],[3]])\n",
    "\n",
    "model=LinearRegressionModel()\n",
    "hypothesis=model(x_train)\n",
    "cost=torch.nn.functional.mse_loss(hypothesis,y_train)\n",
    "\n",
    "print(\"{}.\\n {} {}\\n {} {}\".format(count,x_train,x_train.shape,y_train,y_train.shape)) # 1. 학습데이터\n",
    "count+=1\n",
    "print(\"{}.\\n {}\".format(count,hypothesis)) # 2. y_train의 예측값\n",
    "count+=1\n",
    "print(\"{}. {}\".format(count,cost)) # 3. y_train과 예측값의 오차\n",
    "count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: -0.172, b: 0.355 Cost: 6.139223\n",
      "Epoch  100/1000 W: 0.748, b: 0.573 Cost: 0.047355\n",
      "Epoch  200/1000 W: 0.802, b: 0.451 Cost: 0.029262\n",
      "Epoch  300/1000 W: 0.844, b: 0.354 Cost: 0.018082\n",
      "Epoch  400/1000 W: 0.878, b: 0.278 Cost: 0.011174\n",
      "Epoch  500/1000 W: 0.904, b: 0.219 Cost: 0.006905\n",
      "Epoch  600/1000 W: 0.924, b: 0.172 Cost: 0.004267\n",
      "Epoch  700/1000 W: 0.941, b: 0.135 Cost: 0.002637\n",
      "Epoch  800/1000 W: 0.953, b: 0.106 Cost: 0.001629\n",
      "Epoch  900/1000 W: 0.963, b: 0.084 Cost: 0.001007\n",
      "Epoch 1000/1000 W: 0.971, b: 0.066 Cost: 0.000622\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class LinearRegressionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear=torch.nn.Linear(1,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "\n",
    "x_train=torch.FloatTensor([[1],[2],[3]])\n",
    "y_train=torch.FloatTensor([[1],[2],[3]])\n",
    "\n",
    "model=LinearRegressionModel()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.01)\n",
    "nb_epochs=1000\n",
    "for epoch in range(nb_epochs+1):\n",
    "    prediction=model(x_train)\n",
    "    cost=torch.nn.functional.mse_loss(prediction,y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch%100==0:\n",
    "        params=list(model.parameters())\n",
    "        W=params[0].item()\n",
    "        b=params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f},b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch,nb_epochs,W,b,cost.item()\n",
    "        ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
