{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. tensor([0.0900, 0.2447, 0.6652])\n",
      "2. 1.0\n",
      "3. tensor([[0.2645, 0.1639, 0.1855, 0.2585, 0.1277],\n",
      "        [0.2430, 0.1624, 0.2322, 0.1930, 0.1694],\n",
      "        [0.2226, 0.1986, 0.2326, 0.1594, 0.1868]], grad_fn=<SoftmaxBackward>) torch.Size([3, 5])\n",
      "4. 3.00 [0]: 1.00 [1]: 1.00 [2]: 1.00\n",
      "5. tensor([0, 2, 1]) torch.Size([3])\n",
      " tensor([[0],\n",
      "        [2],\n",
      "        [1]]) torch.Size([3, 1])\n",
      "6. tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.]])\n",
      "7. 1.468920350074768\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "count=1\n",
    "torch.manual_seed(1)\n",
    "z=torch.FloatTensor([1,2,3])\n",
    "hypothesis=torch.nn.functional.softmax(z,dim=0)\n",
    "print(\"{}. {}\".format(count,hypothesis)) # 1. softmax 함수 : e^x_k/e^x_1+...+e^x_n\n",
    "count+=1\n",
    "print(\"{}. {}\".format(count,hypothesis.sum())) # 2. softmax 함수의 합은 1\n",
    "count+=1\n",
    "\n",
    "z=torch.rand(3,5,requires_grad=True)\n",
    "hypothesis=torch.nn.functional.softmax(z,dim=1)\n",
    "print(\"{}. {} {}\".format(count,hypothesis,hypothesis.shape)) # 3. 입력의 형태와 동일\n",
    "count+=1\n",
    "print(\"{}. {:.2f} [0]: {:.2f} [1]: {:.2f} [2]: {:.2f}\".format(count,hypothesis.sum(),\n",
    "hypothesis[0].sum(),hypothesis[1].sum(),hypothesis[2].sum())) # 4. 가장 하위 차원의 합 1\n",
    "count+=1\n",
    "\n",
    "y=torch.randint(5,(3,)).long()  # 입력데이터 결과값\n",
    "print(\"{}. {} {}\\n {} {}\".format(count,y,y.shape,y.unsqueeze(1),y.unsqueeze(1).shape))  # 5.\n",
    "count+=1\n",
    "\n",
    "y_one_hot=torch.zeros_like(hypothesis)\n",
    "y_one_hot.scatter_(1,y.unsqueeze(1),1)\n",
    "print(\"{}. {}\".format(count,y_one_hot)) # 6.\n",
    "count+=1\n",
    "\n",
    "cost=(y_one_hot*-torch.log(hypothesis)).sum(dim=1).mean()\n",
    "print(\"{}. {}\".format(count,cost)) # 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 1.468920350074768\n",
      "2. 1.468920350074768\n",
      "3. 1.468920350074768\n",
      "4. 1.468920350074768\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "count=1\n",
    "torch.manual_seed(1)\n",
    "z=torch.rand(3,5,requires_grad=True)\n",
    "hypothesis=torch.nn.functional.softmax(z,dim=1)\n",
    "y=torch.randint(5,(3,)).long() \n",
    "y_one_hot=torch.zeros_like(hypothesis)\n",
    "y_one_hot.scatter_(1,y.unsqueeze(1),1)\n",
    "\n",
    "cost=(y_one_hot*-torch.log(torch.nn.functional.softmax(z, dim=1))).sum(dim=1).mean()\n",
    "print(\"{}. {}\".format(count,cost)) # 1.\n",
    "count+=1\n",
    "\n",
    "cost=(y_one_hot*-torch.nn.functional.log_softmax(z, dim=1)).sum(dim=1).mean()\n",
    "print(\"{}. {}\".format(count,cost)) # 2.\n",
    "count+=1\n",
    "\n",
    "cost=torch.nn.functional.nll_loss(torch.nn.functional.log_softmax(z, dim=1), y)\n",
    "print(\"{}. {}\".format(count,cost)) # 3.\n",
    "count+=1\n",
    "\n",
    "cost=torch.nn.functional.cross_entropy(z, y)\n",
    "print(\"{}. {}\".format(count,cost)) # 4.\n",
    "count+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.098612\n",
      "Epoch  200/1000 Cost: 0.839114\n",
      "Epoch  400/1000 Cost: 0.788472\n",
      "Epoch  600/1000 Cost: 0.764449\n",
      "Epoch  800/1000 Cost: 0.749399\n",
      "Epoch 1000/1000 Cost: 0.738749\n",
      "0. correct: True prediction: 2 y_train: 2\n",
      "1. correct: True prediction: 2 y_train: 2\n",
      "2. correct: True prediction: 2 y_train: 2\n",
      "3. correct: True prediction: 1 y_train: 1\n",
      "4. correct: False prediction: 0 y_train: 1\n",
      "5. correct: True prediction: 1 y_train: 1\n",
      "6. correct: True prediction: 0 y_train: 0\n",
      "7. correct: True prediction: 0 y_train: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x_train=[[1,2,1,1],[2,1,3,2],[3,1,3,4],[4,1,5,5],[1,7,5,5],[1,2,5,6],[1,6,6,6],[1,7,7,7]]\n",
    "y_train=[2,2,2,1,1,1,0,0]\n",
    "x_train=torch.FloatTensor(x_train)\n",
    "y_train=torch.LongTensor(y_train)\n",
    "\n",
    "W=torch.zeros((4,3),requires_grad=True)\n",
    "b=torch.zeros(1,requires_grad=True)\n",
    "optimizer=torch.optim.SGD([W,b],lr=0.1)\n",
    "nb_epochs=1000\n",
    "ver=2\n",
    "for epoch in range(nb_epochs+1):\n",
    "    z=x_train.matmul(W)+b\n",
    "    # cost ver_1\n",
    "    if ver==1:\n",
    "        hypothesis=torch.nn.functional.softmax(z,dim=1)\n",
    "        y_one_hot=torch.zeros_like(hypothesis)\n",
    "        y_one_hot.scatter_(1,y_train.unsqueeze(1),1)\n",
    "        cost=(y_one_hot*-torch.log(torch.nn.functional.softmax(hypothesis,dim=1))).sum(dim=1).mean()\n",
    "    # cost ver_2\n",
    "    elif ver==2:\n",
    "        hypothesis=torch.nn.functional.softmax(z,dim=1)\n",
    "        cost=torch.nn.functional.cross_entropy(z, y_train) # binary_cross_entropy 와 다르게 softmax를 하지 않고 넣어도 됨\n",
    "    # cost ver_3\n",
    "    elif ver==3:\n",
    "        hypothesis=torch.nn.functional.softmax(z,dim=1)\n",
    "        criterion=torch.nn.CrossEntropyLoss()\n",
    "        cost=criterion(z,y_train) # binary_cross_entropy 와 다르게 softmax를 하지 않고 넣어도 됨\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch%200==0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch,nb_epochs,cost.item()\n",
    "        ))\n",
    "\n",
    "prediction=(hypothesis>=torch.FloatTensor([0.5])).int()\n",
    "prediction=(prediction==1).nonzero(as_tuple=False)[:,1]\n",
    "for i in range(len(prediction)):\n",
    "    correct_prediction=prediction[i]==y_train[i]\n",
    "    print(\"{}. correct: {} prediction: {} y_train: {}\".format(\n",
    "        i,correct_prediction,prediction[i],y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level Implementation with `nn.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.849513\n",
      "Epoch  100/1000 Cost: 0.689894\n",
      "Epoch  200/1000 Cost: 0.609258\n",
      "Epoch  300/1000 Cost: 0.551218\n",
      "Epoch  400/1000 Cost: 0.500141\n",
      "Epoch  500/1000 Cost: 0.451947\n",
      "Epoch  600/1000 Cost: 0.405051\n",
      "Epoch  700/1000 Cost: 0.358734\n",
      "Epoch  800/1000 Cost: 0.312912\n",
      "Epoch  900/1000 Cost: 0.269522\n",
      "Epoch 1000/1000 Cost: 0.241922\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class SoftmaxClassifierModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear=torch.nn.Linear(4,3)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "\n",
    "x_train=[[1,2,1,1],[2,1,3,2],[3,1,3,4],[4,1,5,5],[1,7,5,5],[1,2,5,6],[1,6,6,6],[1,7,7,7]]\n",
    "y_train=[2,2,2,1,1,1,0,0]\n",
    "x_train=torch.FloatTensor(x_train)\n",
    "y_train=torch.LongTensor(y_train)\n",
    "\n",
    "W=torch.zeros((4,3),requires_grad=True)\n",
    "b=torch.zeros(1,requires_grad=True)\n",
    "model=SoftmaxClassifierModel()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.1)\n",
    "nb_epochs=1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    prediction=model(x_train)\n",
    "    cost=torch.nn.functional.cross_entropy(prediction,y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch%200==0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch,nb_epochs,cost.item()\n",
    "        ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
