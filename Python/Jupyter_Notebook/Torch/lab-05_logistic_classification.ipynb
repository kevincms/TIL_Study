{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. e^1=tensor([2.7183])\n",
      "2. 1/(1+e^-1)=tensor([0.7311])\n",
      "3.\n",
      " tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward>)\n",
      "4. 0.6931471824645996\n",
      "5. 0.6931471824645996\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "count=1\n",
    "x_data=[[1,2],[2,3],[3,1],[4,3],[5,3],[6,2]]\n",
    "y_data=[[0],[0],[0],[1],[1],[1]]\n",
    "x_train=torch.FloatTensor(x_data)\n",
    "y_train=torch.FloatTensor(y_data)\n",
    "\n",
    "W=torch.zeros((2,1),requires_grad=True)\n",
    "b=torch.zeros(1,requires_grad=True)\n",
    "\n",
    "hypothesis=1/(1+torch.exp(-(x_train.matmul(W)+b)))\n",
    "print(\"{}. e^1={}\".format(count,torch.exp(torch.FloatTensor([1])))) # 1. e^x 자연상수\n",
    "count+=1\n",
    "hypothesis=torch.sigmoid(x_train.matmul(W)+b)\n",
    "print(\"{}. 1/(1+e^-1)={}\".format(count,torch.sigmoid(torch.FloatTensor([1])))) # 3. sigmoid 함수\n",
    "count+=1\n",
    "print(\"{}.\\n {}\".format(count,hypothesis)) # 3. y_train의 예측값 (1. 과 2. 방식 모두 결과가 동일함)\n",
    "count+=1\n",
    "\n",
    "cost=(-(y_train*torch.log(hypothesis)+(1-y_train)*torch.log(1-hypothesis))).mean()\n",
    "cost=torch.nn.functional.binary_cross_entropy(hypothesis, y_train)\n",
    "print(\"{}. {}\".format(count,cost)) # 4. y_train과 예측값의 오차 (위의 두 방식 모두 결과가 동일함)\n",
    "count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 0.693147\n",
      "Epoch  200/1000 Cost: 0.080643\n",
      "Epoch  400/1000 Cost: 0.045300\n",
      "Epoch  600/1000 Cost: 0.031672\n",
      "Epoch  800/1000 Cost: 0.024394\n",
      "Epoch 1000/1000 Cost: 0.019852\n",
      "0. correct: True prediction: 0 y_train: tensor([0.])\n",
      "1. correct: True prediction: 0 y_train: tensor([0.])\n",
      "2. correct: True prediction: 0 y_train: tensor([0.])\n",
      "3. correct: True prediction: 1 y_train: tensor([1.])\n",
      "4. correct: True prediction: 1 y_train: tensor([1.])\n",
      "5. correct: True prediction: 1 y_train: tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x_data=[[1,2],[2,3],[3,1],[4,3],[5,3],[6,2]]\n",
    "y_data=[[0],[0],[0],[1],[1],[1]]\n",
    "x_train=torch.FloatTensor(x_data)\n",
    "y_train=torch.FloatTensor(y_data)\n",
    "\n",
    "W=torch.zeros((2,1),requires_grad=True)\n",
    "b=torch.zeros(1,requires_grad=True)\n",
    "optimizer=torch.optim.SGD([W,b],lr=1)\n",
    "nb_epochs=1000\n",
    "for epoch in range(nb_epochs+1):\n",
    "    hypothesis=torch.sigmoid(x_train.matmul(W)+b)\n",
    "    cost=torch.nn.functional.binary_cross_entropy(hypothesis, y_train) # cost=-(y_train*torch.log(hypothesis)+(1-y_train)*torch.log(1-hypothesis)).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 200==0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch,nb_epochs,cost.item()))\n",
    "prediction=(hypothesis>=torch.FloatTensor([0.5])).int() # 0.5를 기준으로 학습된 결과를 구분함.\n",
    "for i in range(len(prediction)):\n",
    "    correct_prediction=prediction[i].item()==y_train[i]\n",
    "    print(\"{}. correct: {} prediction: {} y_train: {}\".format(\n",
    "        i,correct_prediction.item(),prediction[i].item(),y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 Cost: 0.829956 Accuracy 34.26%\n",
      "Epoch   20/100 Cost: 0.547120 Accuracy 71.15%\n",
      "Epoch   40/100 Cost: 0.511064 Accuracy 76.81%\n",
      "Epoch   60/100 Cost: 0.494840 Accuracy 76.81%\n",
      "Epoch   80/100 Cost: 0.486423 Accuracy 76.94%\n",
      "Epoch  100/100 Cost: 0.481636 Accuracy 76.55%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class BinaryClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear=torch.nn.Linear(8,1)\n",
    "        self.sigmoid=torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.sigmoid(self.linear(x))\n",
    "\n",
    "xy=np.loadtxt(\"Classification_data.csv\",delimiter=',',dtype=np.float32)\n",
    "x_data=xy[:,0:-1]\n",
    "y_data=xy[:,[-1]]\n",
    "x_train=torch.FloatTensor(x_data)\n",
    "y_train=torch.FloatTensor(y_data)\n",
    "\n",
    "model=BinaryClassifier()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=1)\n",
    "\n",
    "nb_epochs=100\n",
    "for epoch in range(nb_epochs+1):\n",
    "    hypothesis=model(x_train)\n",
    "    cost=torch.nn.functional.binary_cross_entropy(hypothesis,y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch%20==0:\n",
    "        prediction=hypothesis >= torch.FloatTensor([0.5])\n",
    "        correct_prediction=prediction.float()==y_train\n",
    "        accuracy=correct_prediction.sum().item() / len(correct_prediction)\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format(\n",
    "            epoch,nb_epochs,cost.item(),accuracy * 100,\n",
    "        ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
