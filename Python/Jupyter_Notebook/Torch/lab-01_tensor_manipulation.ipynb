{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. tensor([0, 1, 2, 3, 4, 5], dtype=torch.int32)\n",
      "2. tensor([0., 1., 2., 3., 4., 5.])\n",
      "3. 1 1\n",
      "4. torch.Size([6]) torch.Size([6])\n",
      "5. tensor([1, 2, 3, 4], dtype=torch.int32)\n",
      "6. tensor([0., 1., 2., 3., 4., 5.]) tensor([0, 1, 2, 3, 4, 5], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "count=1\n",
    "# tensor 앞의 변수 type에 따라 요소의 type을 바꿀 수 있다. (6.을 이용해 후에 타입을 다시 변경할 수 있다.)\n",
    "int_tensor_1D=torch.IntTensor([i for i in range(6)])\n",
    "float_tensor_1D=torch.FloatTensor([i for i in range(6)])\n",
    "print(\"{}. {}\".format(count,int_tensor_1D)) # 1. type=int\n",
    "count+=1\n",
    "print(\"{}. {}\".format(count,float_tensor_1D)) # 2 type=float\n",
    "count+=1\n",
    "print(\"{}. {} {}\".format(count,int_tensor_1D.dim(),int_tensor_1D.ndim)) # 3. tensor의 차원\n",
    "count+=1\n",
    "print(\"{}. {} {}\".format(count,int_tensor_1D.shape,int_tensor_1D.size())) # 4. tensor의 형태\n",
    "count+=1\n",
    "print(\"{}. {}\".format(count,int_tensor_1D[1:-1])) # 5. list와 같이 slice 가능 [주의 2차원부터는 다름]\n",
    "count+=1\n",
    "print(\"{}. {} {}\".format(count,int_tensor_1D.float(),float_tensor_1D.int())) # 6. tensor의 타입 변경\n",
    "count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D, 3D Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\n",
      " tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "2.\n",
      " tensor([[[ 0.,  1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.,  7.],\n",
      "         [ 8.,  9., 10., 11.]],\n",
      "\n",
      "        [[12., 13., 14., 15.],\n",
      "         [16., 17., 18., 19.],\n",
      "         [20., 21., 22., 23.]]])\n",
      "3. 2 3\n",
      "4. torch.Size([3, 4]) torch.Size([2, 3, 4])\n",
      "5. tensor([1, 5, 9])\n",
      "6.\n",
      " tensor([[[ 0.],\n",
      "         [ 4.],\n",
      "         [ 8.]],\n",
      "\n",
      "        [[12.],\n",
      "         [16.],\n",
      "         [20.]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "count=1\n",
    "# reshape와 view를 통해 tensor의 차원(형태)를 바꿀 수 있다.\n",
    "int_tensor_2D=torch.reshape(torch.arange(12),(3,4))\n",
    "float_tensor_3D=torch.arange(24,dtype=torch.float).view(-1,3,4) # -1을 넣으면 알아서 계산해준다.\n",
    "print(\"{}.\\n {}\".format(count,int_tensor_2D)) # 1.\n",
    "count+=1\n",
    "print(\"{}.\\n {}\".format(count,float_tensor_3D)) # 2. \n",
    "count+=1\n",
    "print(\"{}. {} {}\".format(count,int_tensor_2D.dim(), float_tensor_3D.ndim)) # 3. tensor의 차원\n",
    "count+=1\n",
    "print(\"{}. {} {}\".format(count,int_tensor_2D.shape, float_tensor_3D.size())) # 4. tensor의 형태\n",
    "count+=1\n",
    "print(\"{}. {}\".format(count,int_tensor_2D[:,1])) # 5. 2차원 이상 : tensor=[,] list=[][] \n",
    "count+=1\n",
    "print(\"{}.\\n {}\".format(count,float_tensor_3D[:,:3,:1])) # 6.\n",
    "count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero & One Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. tensor([0., 0., 0., 0., 0., 0.])\n",
      "2.\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "3.\n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "4. tensor([1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "count=1\n",
    "zero_tensor=torch.zeros(6)\n",
    "one_tensor=torch.ones(2,3)\n",
    "\n",
    "print(\"{}. {}\".format(count,zero_tensor)) # 1.\n",
    "count+=1\n",
    "print(\"{}.\\n {}\".format(count,one_tensor)) # 2.\n",
    "count+=1\n",
    "print(\"{}.\\n {}\".format(count,torch.zeros_like(one_tensor))) # 3. \n",
    "count+=1\n",
    "print(\"{}. {}\".format(count,torch.ones_like(zero_tensor))) # 4.\n",
    "count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\n",
      " tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "2.\n",
      " tensor([[1.],\n",
      "        [2.]])\n",
      "3.\n",
      " tensor([[ 5.],\n",
      "        [11.]])\n",
      " tensor([[ 5.],\n",
      "        [11.]])\n",
      "4.\n",
      " tensor([[1., 2.],\n",
      "        [6., 8.]])\n",
      "5.\n",
      " tensor([[1., 2.],\n",
      "        [6., 8.]])\n",
      "6.\n",
      " tensor([[2., 3.],\n",
      "        [5., 6.]])\n",
      "7.\n",
      " tensor([[0., 1.],\n",
      "        [1., 2.]])\n",
      "8.\n",
      " tensor([[1.0000, 2.0000],\n",
      "        [1.5000, 2.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "count=1\n",
    "\n",
    "matrix_1 = torch.FloatTensor([i+1 for i in range(4)]).reshape(2,2)\n",
    "matrix_2 = torch.FloatTensor([[1], [2]])\n",
    "print(\"{}.\\n {}\".format(count,matrix_1)) # 1. \n",
    "count+=1\n",
    "print(\"{}.\\n {}\".format(count,matrix_2)) # 2. \n",
    "count+=1\n",
    "print(\"{}.\\n {}\\n {}\".format(count,matrix_1.matmul(matrix_2),matrix_1@matrix_2)) # 3. 행렬 곱\n",
    "count+=1\n",
    "# 기본적인 사칙연산은 자동으로 tensor의 크기를 조절한다.\n",
    "print(\"{}.\\n {}\".format(count,matrix_1.mul(matrix_2))) # 4. 일반 곱 ver_1 | .mul_()시 matrix_1 값이 바뀜\n",
    "count+=1\n",
    "print(\"{}.\\n {}\".format(count,matrix_1*matrix_2)) # 5. 일반 곱 ver_2\n",
    "count+=1\n",
    "print(\"{}.\\n {}\".format(count,matrix_1+matrix_2)) # 6. 일반 합\n",
    "count+=1\n",
    "print(\"{}.\\n {}\".format(count,matrix_1-matrix_2)) # 7. 일반 차\n",
    "count+=1\n",
    "print(\"{}.\\n {}\".format(count,matrix_1/matrix_2)) # 8. 일반 나누기\n",
    "count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squeeze & Unsqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. tensor([[0, 1]]) torch.Size([1, 2])\n",
      "2. tensor([0, 1]) torch.Size([2])\n",
      "3. tensor([[0, 1]]) torch.Size([1, 2])\n",
      "4.\n",
      " tensor([[0],\n",
      "        [1]]) torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "count=1\n",
    "# Squeeze는 size 중 하나가 1일 떄 사용가능\n",
    "test_tensor=torch.arange(2).view(-1,2)\n",
    "print(\"{}. {} {}\".format(count,test_tensor,test_tensor.shape)) # 1. Squeeze 전 행렬\n",
    "count+=1\n",
    "test_tensor=test_tensor.squeeze()\n",
    "print(\"{}. {} {}\".format(count,test_tensor,test_tensor.shape)) # 2. Squeeze 후 행렬\n",
    "count+=1\n",
    "\n",
    "temp_tensor=test_tensor.unsqueeze(0)\n",
    "print(\"{}. {} {}\".format(count,temp_tensor,temp_tensor.shape)) # 3. 차원 0에 Unsqueeze 후 행렬\n",
    "count+=1\n",
    "\n",
    "temp_tensor=test_tensor.unsqueeze(1)\n",
    "print(\"{}.\\n {} {}\".format(count,temp_tensor,temp_tensor.shape)) # 4. 차원 1에 Unsqueeze 후 행렬\n",
    "count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation & Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\n",
      " tensor([[1., 2.],\n",
      "        [3., 4.]]) torch.Size([2, 2])\n",
      "2.\n",
      " tensor([[5., 6.],\n",
      "        [7., 8.]]) torch.Size([2, 2])\n",
      "3.\n",
      " tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.],\n",
      "        [7., 8.]]) torch.Size([4, 2])\n",
      "4.\n",
      " tensor([[1., 2., 5., 6.],\n",
      "        [3., 4., 7., 8.]]) torch.Size([2, 4])\n",
      "5. tensor([1., 4.]) tensor([2., 5.]) tensor([3., 6.]) torch.Size([2])\n",
      "6.\n",
      " tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]]) torch.Size([3, 2])\n",
      "7.\n",
      " tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]]) torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "count=1\n",
    "\n",
    "test_tensor_1=torch.FloatTensor([[1, 2], [3, 4]])\n",
    "test_tensor_2=torch.FloatTensor([[5, 6], [7, 8]])\n",
    "print(\"{}.\\n {} {}\".format(count,test_tensor_1,test_tensor_1.shape)) # 1.\n",
    "count+=1\n",
    "print(\"{}.\\n {} {}\".format(count,test_tensor_2,test_tensor_2.shape)) # 2.\n",
    "count+=1\n",
    "\n",
    "temp_tensor=torch.cat([test_tensor_1, test_tensor_2], dim=0)\n",
    "print(\"{}.\\n {} {}\".format(count,temp_tensor,temp_tensor.shape)) # 3. 차원 0에 Concatenation 후 행렬\n",
    "count+=1\n",
    "temp_tensor=torch.cat([test_tensor_1, test_tensor_2], dim=1)\n",
    "print(\"{}.\\n {} {}\".format(count,temp_tensor,temp_tensor.shape)) # 4. 차원 1에 Concatenation 후 행렬\n",
    "count+=1\n",
    "\n",
    "x = torch.FloatTensor([1, 4])\n",
    "y = torch.FloatTensor([2, 5])\n",
    "z = torch.FloatTensor([3, 6])\n",
    "\n",
    "print(\"{}. {} {} {} {}\".format(count,x,y,z,z.shape)) # 5.\n",
    "count+=1\n",
    "temp_tensor=torch.stack([x, y, z])\n",
    "print(\"{}.\\n {} {}\".format(count,temp_tensor,temp_tensor.shape)) # 6. 차원 0에 Stacking 후 행렬\n",
    "count+=1\n",
    "temp_tensor=torch.stack([x, y, z], dim=1)\n",
    "print(\"{}.\\n {} {}\".format(count,temp_tensor,temp_tensor.shape)) # 7. 차원 1에 Stacking 후 행렬"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
